{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MaintOperationDetailDesc</th>\n",
       "      <th>CorroPerc</th>\n",
       "      <th>DESCRIPTION_x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.40</td>\n",
       "      <td>117-Deteriorated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.40</td>\n",
       "      <td>117-Deteriorated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COMPLETED. REPLACED IN CONJUNCTION WITH AXIAL ...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>170-Corroded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COMPLETED</td>\n",
       "      <td>0.00</td>\n",
       "      <td>117-Deteriorated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>REINSTALLED INTERMEDIATE GEARBOX ACCELEROMETER...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>117-Deteriorated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>REINSTALLED</td>\n",
       "      <td>0.00</td>\n",
       "      <td>170-Corroded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>RECONNECTED 4EA ELECTRICAL CONNECTORS TO DEICE...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>170-Corroded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>492</th>\n",
       "      <td>COMPLETED OK</td>\n",
       "      <td>0.05</td>\n",
       "      <td>170-Corroded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>493</th>\n",
       "      <td>INSPECTION COMPLETE CHECKS FOUND OK</td>\n",
       "      <td>0.33</td>\n",
       "      <td>170-Corroded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494</th>\n",
       "      <td>REINSTALLED</td>\n",
       "      <td>0.00</td>\n",
       "      <td>050-Blistered</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>495 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              MaintOperationDetailDesc  CorroPerc  \\\n",
       "0                                            COMPLETED       0.40   \n",
       "1                                            COMPLETED       0.40   \n",
       "2    COMPLETED. REPLACED IN CONJUNCTION WITH AXIAL ...       0.00   \n",
       "3                                            COMPLETED       0.00   \n",
       "4    REINSTALLED INTERMEDIATE GEARBOX ACCELEROMETER...       0.33   \n",
       "..                                                 ...        ...   \n",
       "490                                        REINSTALLED       0.00   \n",
       "491  RECONNECTED 4EA ELECTRICAL CONNECTORS TO DEICE...       0.00   \n",
       "492                                       COMPLETED OK       0.05   \n",
       "493                INSPECTION COMPLETE CHECKS FOUND OK       0.33   \n",
       "494                                        REINSTALLED       0.00   \n",
       "\n",
       "        DESCRIPTION_x  \n",
       "0    117-Deteriorated  \n",
       "1    117-Deteriorated  \n",
       "2        170-Corroded  \n",
       "3    117-Deteriorated  \n",
       "4    117-Deteriorated  \n",
       "..                ...  \n",
       "490      170-Corroded  \n",
       "491      170-Corroded  \n",
       "492      170-Corroded  \n",
       "493      170-Corroded  \n",
       "494     050-Blistered  \n",
       "\n",
       "[495 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bring in matching records for evaluation of the Keras_1D classifier \n",
    "df = pd.read_csv('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/record_linkage/obj2_recs.csv', encoding='utf-8')\n",
    "df[['MaintOperationDetailDesc', 'CorroPerc', 'DESCRIPTION_x']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "321\n",
      "182\n"
     ]
    }
   ],
   "source": [
    "# make the y_MADW column an indicator for 1 = corrosion-tag, 0 = no corrosion\n",
    "y = []\n",
    "for row in df.DESCRIPTION_x:\n",
    "    if row in ['170-Corroded', '520-Pitted', '050-Blistered', '240-Flaking']:\n",
    "        y.append(1)\n",
    "    else:\n",
    "        y.append(0)\n",
    "        \n",
    "# make the y_MADW column an indicator for 1 = corrosion-tag, 0 = no corrosion\n",
    "y_MADW = []\n",
    "for row in df.CorroPerc:\n",
    "    if row !=0:\n",
    "        y_MADW.append(1)\n",
    "    else:\n",
    "        y_MADW.append(0)\n",
    "        \n",
    "print(sum(y))\n",
    "print(sum(y_MADW))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y'] = y\n",
    "df['y_MADW'] = y_MADW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['completed',\n",
       " 'completed',\n",
       " 'completed replaced conjunction axial fan replacement',\n",
       " 'completed',\n",
       " 'reinstalled intermediate gearbox accelerometers brackets',\n",
       " 'carried ##### event #### dated 5 june ####',\n",
       " 'check complete',\n",
       " 'comp passed ck iaw em #### wp #### ## mr ##### tq ##### lbs',\n",
       " 'complete',\n",
       " 'completed']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocess text just like I did for training the convnet\n",
    "\n",
    "# Ensure that text variables are strings\n",
    "df.MaintOperationDetailDesc = df.MaintOperationDetailDesc.astype(str)\n",
    "\n",
    "\n",
    "# Remove special characters\n",
    "def clean_text(x):\n",
    "    pattern = r'[^a-zA-z0-9\\s]'\n",
    "    text = re.sub(pattern, '', x)\n",
    "    return text\n",
    "\n",
    "cleaned_text = []\n",
    "for text in df['MaintOperationDetailDesc']:\n",
    "    cleaned_text.append(clean_text(text.lower()))\n",
    "\n",
    "# Replace contractions\n",
    "contraction_dict = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "def _get_contractions(contraction_dict):\n",
    "    contraction_re = re.compile('(%s)' % '|'.join(contraction_dict.keys()))\n",
    "    return contraction_dict, contraction_re\n",
    "\n",
    "contractions, contractions_re = _get_contractions(contraction_dict)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    def replace(match):\n",
    "        return contractions[match.group(0)]\n",
    "    return contractions_re.sub(replace, text)\n",
    "\n",
    "# Usage\n",
    "#replace_contractions(\"this's a text with contraction\")\n",
    "\n",
    "contr_cleaned_text = []\n",
    "for text in cleaned_text:\n",
    "    contr_cleaned_text.append(replace_contractions(text))\n",
    "\n",
    "# remove numbers\n",
    "def clean_numbers(x):\n",
    "    if bool(re.search(r'\\d', x)):\n",
    "        x = re.sub('[0-9]{5,}', '#####', x)\n",
    "        x = re.sub('[0-9]{4}', '####', x)\n",
    "        x = re.sub('[0-9]{3}', '###', x)\n",
    "        x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "num_contr_cleaned_text = []\n",
    "for text in contr_cleaned_text:\n",
    "    num_contr_cleaned_text.append(clean_numbers(text))\n",
    "\n",
    "#num_contr_cleaned_text[0:5]\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "x_no_stp_wds = []\n",
    "for doc in num_contr_cleaned_text:\n",
    "    filtered_words = []\n",
    "    for word in doc.split():\n",
    "        if word.lower() not in stopwords.words('english'):\n",
    "            filtered_words.append(word.lower())\n",
    "        else:\n",
    "            next\n",
    "    x_no_stp_wds.append(' '.join(filtered_words)) #joins the words into a sentence, appends the sentence as list to x_no_stp_wds\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# from joblib import Parallel, delayed\n",
    "# import multiprocessing\n",
    "\n",
    "# def remove_stp_wds(doc, stp_wds= stopwords.words('english')):\n",
    "#     filtered_words = []\n",
    "#     for word in doc.split():\n",
    "#         if word.lower() not in stp_wds:\n",
    "#             filtered_words.append(word.lower())\n",
    "#         else:\n",
    "#             next\n",
    "#     return(' '.join(filtered_words)) \n",
    "\n",
    "# num_cores = multiprocessing.cpu_count()\n",
    "# x_no_stp_wds = Parallel(n_jobs=num_cores)(delayed(remove_stp_wds)(doc) for doc in num_contr_cleaned_text)\n",
    "\n",
    "x_no_stp_wds[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# create the tokenizer\n",
    "t = Tokenizer()\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(x_no_stp_wds)\n",
    "# summarize what was learned\n",
    "#Once fit, the Tokenizer provides 4 attributes that you can use to query what has been learned about your documents:\n",
    "\n",
    "#word_counts: A dictionary of words and their counts.\n",
    "#word_docs: A dictionary of words and how many documents each appeared in.\n",
    "#word_index: A dictionary of words and their uniquely assigned integers.\n",
    "#document_count:An integer count of the total number of documents that were used to fit the Tokenizer\n",
    "#print(t.word_counts)\n",
    "print(t.document_count) # total number of docs\n",
    "len(t.word_index) # total number of words in the vocab (originally 94819 without removing stopwords)\n",
    "#print(t.word_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters:\n",
    "max_features = 10000 # cut down the vocab size\n",
    "maxlen = 400 # cut down the max document length\n",
    "\n",
    "# integer encode the documents\n",
    "docs = x_no_stp_wds\n",
    "labels = df['y']\n",
    "\n",
    "encoded_docs = [one_hot(d, max_features) for d in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495 test sequences\n"
     ]
    }
   ],
   "source": [
    "x_test = encoded_docs\n",
    "y_test = labels\n",
    "print(len(x_test), 'test sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_test shape: (495, 400)\n"
     ]
    }
   ],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "print('x_test shape:', x_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1DConv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mh302/opt/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/corr_model_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_model_1DConv_pretrnd_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.402020\n",
      "Precision: 0.564767\n",
      "Recall: 0.339564\n",
      "F1 score: 0.424125\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " \n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>yhat_classes</th>\n",
       "      <th>y_MADW</th>\n",
       "      <th>MaintOperationDetailDesc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED. REPLACED IN CONJUNCTION WITH AXIAL ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>COMPLETED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>CHECK COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>COMP PASSED CK IAW EM 0284 WP 1556 00 M/R 1264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>INSPECTION FOUND OK TQ S/N 5595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>REPLACED. TQ = 250 FT. LBS. S/N: B1230 (18 APR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>RECONNECTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A, NBIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>COMPLETE IAW EM0013 WP 0572 00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>COMP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     y  yhat_classes  y_MADW  \\\n",
       "2    1             1       0   \n",
       "3    0             1       0   \n",
       "6    1             0       1   \n",
       "7    1             0       1   \n",
       "10   0             0       1   \n",
       "..  ..           ...     ...   \n",
       "478  1             1       0   \n",
       "479  0             1       0   \n",
       "484  1             1       0   \n",
       "486  0             0       1   \n",
       "488  1             1       0   \n",
       "\n",
       "                              MaintOperationDetailDesc  \n",
       "2    COMPLETED. REPLACED IN CONJUNCTION WITH AXIAL ...  \n",
       "3                                            COMPLETED  \n",
       "6                                       CHECK COMPLETE  \n",
       "7    COMP PASSED CK IAW EM 0284 WP 1556 00 M/R 1264...  \n",
       "10                     INSPECTION FOUND OK TQ S/N 5595  \n",
       "..                                                 ...  \n",
       "478  REPLACED. TQ = 250 FT. LBS. S/N: B1230 (18 APR...  \n",
       "479                                       RECONNECTED   \n",
       "484                                          N/A, NBIT  \n",
       "486                     COMPLETE IAW EM0013 WP 0572 00  \n",
       "488                                               COMP  \n",
       "\n",
       "[203 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['yhat_classes'] = yhat_classes\n",
    "df[['y', 'yhat_classes', 'y_MADW', 'MaintOperationDetailDesc']][yhat_classes!=y_MADW].to_csv('error_analysis.csv', index=True)\n",
    "df[['y', 'yhat_classes', 'y_MADW', 'MaintOperationDetailDesc']][yhat_classes!=y_MADW]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of  495  matching records from MADW to ASAP-RAM \"matches\"  \n",
    "Proportion of \"matching\" records with a corr match: 0.416  \n",
    "MADW Expert System Performance:  \n",
    "Accuracy: 0.416162  \n",
    "Precision: 0.587912  \n",
    "Recall: 0.333333  \n",
    "F1 score: 0.425447  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.402020202020202\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3588272699450845, 0.4452131340953195)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1DConv Model Pretrained Trainable Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mh302/opt/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/corr_model_1DConv_pretrnd.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.648485\n",
      "Precision: 0.648485\n",
      "Recall: 1.000000\n",
      "F1 score: 0.786765\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " \n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of  495  matching records from MADW to ASAP-RAM \"matches\"  \n",
    "Proportion of \"matching\" records with a corr match: 0.416  \n",
    "MADW Expert System Performance:  \n",
    "Accuracy: 0.416162  \n",
    "Precision: 0.587912  \n",
    "Recall: 0.333333  \n",
    "F1 score: 0.425447  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on F1 by:  84.93490376004532 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on F1 Score\n",
    "print(\"DL model improvement on F1 by: \", abs(0.425447 - 0.7868) / 0.425447 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on accuracy by:  55.82873976960895 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on accuracy\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.416162 - 0.6485) /  0.416162 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6484848484848484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6064250262356793, 0.6905446707340176)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 1D Conv Nets with pretrained embeddings (both trainable and un-trainable) resulted in the same model!  This means that the pre-trained embeddings were optimal for the 1D Convnet task (doc classification).  Very interesting.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1DConv Model Pretrained Fixed Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/corr_model_1DConv_pretrnd_fixed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.648485\n",
      "Precision: 0.648485\n",
      "Recall: 1.000000\n",
      "F1 score: 0.786765\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = model.predict_classes(x_test, verbose=0)\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "yhat_classes = yhat_classes[:, 0]\n",
    " \n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of  495  matching records from MADW to ASAP-RAM \"matches\"  \n",
    "Proportion of \"matching\" records with a corr match: 0.416  \n",
    "MADW Expert System Performance:  \n",
    "Accuracy: 0.416162  \n",
    "Precision: 0.587912  \n",
    "Recall: 0.333333  \n",
    "F1 score: 0.425447  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on F1 by:  84.93490376004532 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on F1 Score\n",
    "print(\"DL model improvement on F1 by: \", abs(0.425447 - 0.7868) / 0.425447 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on accuracy by:  55.82873976960895 %\n",
      "DL model improvement on accuracy by:  200.30030030030028 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on accuracy\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.416162 - 0.6485) /  0.416162 * 100, \"%\")\n",
    "\n",
    "# improvement on recall\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.333 - 1) /  0.333 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6484848484848484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6064250262356793, 0.6905446707340176)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mh302/opt/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/atn_model_norm_corpus_unique.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.549495\n",
      "Precision: 0.745000\n",
      "Recall: 0.464174\n",
      "F1 score: 0.571977\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model_lstm.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = [round(pred[0]) for pred in yhat_probs]\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5494949494949495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5056643736782512, 0.5933255253116477)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on F1 by:  34.44142278591691 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on F1 Score\n",
    "print(\"DL model improvement on F1 by: \", abs(0.425447 - 0.571977) / 0.425447 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on accuracy by:  32.03872530408831 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on accuracy\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.416162 - 0.549495) /  0.416162 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model with trainable pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/Users/mh302/opt/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/atn_model_norm_corpus_unique_pretrn_emb.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.444444\n",
      "Precision: 0.632184\n",
      "Recall: 0.342679\n",
      "F1 score: 0.444444\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model_lstm.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = [round(pred[0]) for pred in yhat_probs]\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.40067026842202497, 0.4882186204668639)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on F1 by:  4.465280046633305 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on F1 Score\n",
    "print(\"DL model improvement on F1 by: \", abs(0.425447 - 0.4444444) / 0.425447 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on accuracy by:  6.796007324070925 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on accuracy\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.416162 - 0.4444444) /  0.416162 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apparently, using pre-trained embeddings makes the LSTM perform more similarly to the expert system model.  ie we are making the DL model use the human (expert) understanding of words vs allowing the machine to learn from experience on the matinenance data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model with fixed pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model_lstm = load_model('/Users/mh302/OneDrive - West Point/Backup/ORCEN/CPO/deep_learning/atn_model_norm_corpus_unique_pretrn_emb_fixed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.658586\n",
      "Precision: 0.674312\n",
      "Recall: 0.915888\n",
      "F1 score: 0.776750\n"
     ]
    }
   ],
   "source": [
    "# https://machinelearningmastery.com/how-to-calculate-precision-recall-f1-and-more-for-deep-learning-models/ \n",
    "# predict probabilities for test set\n",
    "yhat_probs = model_lstm.predict(x_test, verbose=0)\n",
    "# predict crisp classes for test set\n",
    "yhat_classes = [round(pred[0]) for pred in yhat_probs]\n",
    "# reduce to 1d array\n",
    "yhat_probs = yhat_probs[:, 0]\n",
    "\n",
    "# accuracy: (tp + tn) / (p + n), the ratio of correctly predicted observation to the total observations.\n",
    "accuracy = accuracy_score(y_test, yhat_classes)\n",
    "print('Accuracy: %f' % accuracy)\n",
    "# precision tp / (tp + fp), the ratio of correctly predicted positive observations to the total predicted positive observations\n",
    "# If precision is high, we are making correct corrosion tags, but possibly missing corrosion items\n",
    "precision = precision_score(y_test, yhat_classes)\n",
    "print('Precision: %f' % precision)\n",
    "# recall: tp / (tp + fn), ratio of correctly predicted positive observations to the all observations in actual class\n",
    "# So if recall is low, we are missing actual corrosion actions\n",
    "recall = recall_score(y_test, yhat_classes)\n",
    "print('Recall: %f' % recall)\n",
    "# f1: 2 tp / (2 tp + fp + fn), the weighted average of Precision and Recall\n",
    "# If this is high, we are making correct corrosion tags and not missing actions that should be corrosion.\n",
    "f1 = f1_score(y_test, yhat_classes)\n",
    "print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4444444444444444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.40067026842202497, 0.4882186204668639)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = len(y_test[y_test == yhat_classes])\n",
    "nobs = len(y_test)\n",
    "print(num_correct/nobs)\n",
    "from statsmodels.stats import proportion as prop\n",
    "# compute 95% CI on model accuracy\n",
    "match_prop = prop.proportion_confint(count=num_correct, nobs=nobs, alpha=0.05, method='normal')\n",
    "match_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on F1 by:  82.57268237876869 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on F1 Score\n",
    "print(\"DL model improvement on F1 by: \", abs(0.425447 - 0.77675) / 0.425447 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DL model improvement on accuracy by:  58.25567927874241 %\n"
     ]
    }
   ],
   "source": [
    "# improvement on accuracy\n",
    "print(\"DL model improvement on accuracy by: \", abs( 0.416162 - 0.6586) /  0.416162 * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing the pre-trained embeddings made the model a poorer performer on the train/val set, but big improvement on the ASAP-RAM test set! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras_env",
   "language": "python",
   "name": "keras_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
